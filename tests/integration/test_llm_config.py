"""
Test script for LLM Configuration System
"""
import requests
import json
import time
import sys

def test_llm_config():
    """Test the LLM configuration system"""
    
    print("ğŸ§ª LLM Configuration System Test")
    print("=" * 50)
    
    base_url = "http://127.0.0.1:5000"
    
    # Test server connectivity
    try:
        response = requests.get(f"{base_url}/", timeout=5)
        if response.status_code != 200:
            print(f"âŒ Server not responding: {response.status_code}")
            return False
        print("âœ… Server is running")
        
    except Exception as e:
        print(f"âŒ Cannot connect to server: {e}")
        print("ğŸ’¡ Make sure the server is running on port 5000")
        return False
    
    # Create enrollment
    print("\nğŸ” Creating enrollment...")
    enrollment_data = {
        "id": "kali-lab-01",
        "token": "test-token-for-llm-config-123456789",
        "label": "LLM Configuration Test"
    }
    
    try:
        enroll_response = requests.post(f"{base_url}/enroll", json=enrollment_data, timeout=10)
        
        if enroll_response.status_code != 200:
            print(f"âŒ Enrollment failed: {enroll_response.status_code}")
            print(f"ğŸ“‹ Response: {enroll_response.text}")
            return False
            
        credentials = enroll_response.json()
        api_key = credentials["api_key"]
        server_id = credentials["server_id"]
        
        print(f"âœ… Enrolled server: {server_id}")
        print(f"ğŸ”‘ API Key: {api_key[:20]}...")
        
    except Exception as e:
        print(f"âŒ Enrollment error: {e}")
        return False
    
    # Test LLM configuration endpoints
    headers = {"Authorization": f"Bearer {api_key}"}
    
    print(f"\nğŸ“‹ Testing LLM Configuration Endpoints...")
    
    # Get default configuration
    try:
        config_response = requests.get(f"{base_url}/llm/config", headers=headers, timeout=5)
        
        if config_response.status_code != 200:
            print(f"âŒ Failed to get LLM config: {config_response.status_code}")
            return False
            
        current_config = config_response.json()
        print("âœ… Retrieved default LLM configuration")
        print(f"ğŸ“‹ Current ETag: {current_config.get('etag', 'none')}")
        print(f"ğŸ“‹ System Prompt: {current_config.get('system_prompt', '')[:100]}...")
        
    except Exception as e:
        print(f"âŒ Error getting LLM config: {e}")
        return False
    
    # Apply your target configuration
    print(f"\nğŸ”§ Applying target LLM configuration...")
    
    target_config = {
        "system_prompt": "You are the MCP Server Assistant for Kali-Lab-01. Use only provided memory, knowledge, and live context. Respond with explicit, copy-pastable steps. If action is risky, ask for confirmation and suggest a dry run. Keep answers under 200 tokens unless logs are requested.",
        "guardrails": {
            "disallowed": ["secrets", "credentials", "api_keys"], 
            "style": "concise"
        },
        "runtime_hints": {
            "preferred_model": "phi3:mini", 
            "num_ctx": 768, 
            "temperature": 0.2, 
            "num_gpu": 0, 
            "keep_alive": 0
        },
        "tools_allowed": ["nmap-scan", "cme-enum", "zap-active"]
    }
    
    try:
        # Use ETag for optimistic concurrency
        update_headers = {**headers, "If-Match": current_config.get("etag", "")}
        
        update_response = requests.put(
            f"{base_url}/llm/config",
            json=target_config,
            headers=update_headers,
            timeout=10
        )
        
        if update_response.status_code != 200:
            print(f"âŒ Failed to update LLM config: {update_response.status_code}")
            print(f"ğŸ“‹ Response: {update_response.text}")
            return False
            
        updated_config = update_response.json()
        print("âœ… Successfully updated LLM configuration!")
        print(f"ğŸ“‹ New ETag: {updated_config.get('etag', 'none')}")
        
        # Verify the configuration was applied
        verify_response = requests.get(f"{base_url}/llm/config", headers=headers, timeout=5)
        if verify_response.status_code == 200:
            verified_config = verify_response.json()
            
            print(f"\nğŸ“‹ Configuration Applied Successfully:")
            print(f"   ğŸ¯ Server ID: {verified_config['server_id']}")
            print(f"   ğŸ¤– Model: {verified_config['runtime_hints'].get('preferred_model', 'not set')}")
            print(f"   ğŸŒ¡ï¸  Temperature: {verified_config['runtime_hints'].get('temperature', 'not set')}")
            print(f"   ğŸ› ï¸  Tools Allowed: {len(verified_config.get('tools_allowed', []))} tools")
            print(f"   ğŸ›¡ï¸  Guardrails: {list(verified_config.get('guardrails', {}).keys())}")
            
            # Show the actual configuration
            print(f"\nğŸ“ Full Configuration:")
            print(json.dumps(verified_config, indent=2))
            
        return True
        
    except Exception as e:
        print(f"âŒ Error updating LLM config: {e}")
        return False
    
    # Test other LLM endpoints
    print(f"\nğŸ§  Testing other LLM endpoints...")
    
    # Test JWT token creation
    try:
        token_request = {"api_key": api_key}
        token_response = requests.post(f"{base_url}/auth/token", json=token_request, timeout=5)
        
        if token_response.status_code == 200:
            token_data = token_response.json()
            print(f"âœ… JWT token created successfully")
            print(f"ğŸ”‘ Token type: {token_data.get('token_type', 'unknown')}")
            print(f"â° Expires in: {token_data.get('expires_in', 0)} seconds")
        else:
            print(f"âŒ JWT token creation failed: {token_response.status_code}")
            
    except Exception as e:
        print(f"âŒ JWT token error: {e}")
    
    # Test live context
    try:
        context_response = requests.get(f"{base_url}/llm/context", headers=headers, timeout=5)
        
        if context_response.status_code == 200:
            context_data = context_response.json()
            print(f"âœ… Live context retrieved")
            print(f"ğŸ“Š Uptime: {context_data.get('uptime', 'unknown')}")
            print(f"ğŸ’½ Disk usage: {context_data.get('disk_usage', 'unknown')}")
            print(f"ğŸš¨ Alerts: {len(context_data.get('alerts', []))}")
        else:
            print(f"âŒ Live context failed: {context_response.status_code}")
            
    except Exception as e:
        print(f"âŒ Live context error: {e}")
    
    print(f"\nğŸ‰ LLM Configuration System Test Complete!")
    return True

if __name__ == "__main__":
    success = test_llm_config()
    if success:
        print(f"\nâœ… All tests passed!")
        sys.exit(0)
    else:
        print(f"\nâŒ Some tests failed!")
        sys.exit(1)